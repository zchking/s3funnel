#!/usr/bin/env python
# s3funnel - Multithreaded tool for performing operations on Amazon's S3
# Copyright (c) 2008 Andrey Petrov
#
# This module is part of s3funnel and is released under
# the MIT license: http://www.opensource.org/licenses/mit-license.php

"""
s3funnel is a multithreaded tool for performing operations on Amazon's S3.
"""

import os
import workerpool
import boto
import logging

from glob import glob

import signal
import threading

from optparse import OptionParser

log = logging.getLogger(__name__)
event_stop = threading.Event()

class S3ToolBox(object):
    def __init__(self, aws_key, aws_secret_key, bucket):
        self.conn = boto.connect_s3(aws_key, aws_secret_key)
        self.bucket = self.conn.get_bucket(bucket)

# Multithreaded jobs

class GetJob(workerpool.Job):
    def __init__(self, key):
        self.key = key

    def run(self, toolbox):
        k = toolbox.bucket.new_key(self.key)
        try:
            # FIXME: Don't create file if it didn't fetch properly
            k.get_contents_to_filename(self.key)
            log.info("Got: %s" % self.key)
        except boto.exception.S3ResponseError:
            log.error("Failed to get: %s" % self.key)

# Singlethreaded methods

def perform_list(toolbox, start_key=None):
    for i in toolbox.bucket:
        print i.name

def main():
    # Parse the command line...
    usage="%prog BUCKET OPERATION [OPTIONS] [FILE]...\n" + __doc__
    parser = OptionParser(usage)
    parser.add_option("-a", "--aws_key", dest="aws_key", type="string", 
                        help="Overrides AWS_ACCESS_KEY_ID environment variable")
    parser.add_option("-s", "--aws_secret_key", dest="aws_secret_key", type="string",
                        help="Overrides AWS_SECRET_ACCESS_KEY environment variable")
    parser.add_option("-t", "--threads", dest="numthreads", default=1, type="int", metavar="N",
                        help="Number of threads to use (default: 5)")
    parser.add_option("--start_key", dest="start_key", type="string", default=None,
                        help="(`list` only) Start key for list operation")
    parser.add_option("--acl", dest="acl", type="string", default="public-read",
                        help="(`put` only) Set the ACL permission for each file (default: public-read)")
    parser.add_option("-i", "--input", dest="input", type="string", metavar="FILE",
                        help="Read one file per line from a FILE manifest.")
    parser.add_option("-v", "--verbose", action="store_true", dest="verbose", default=None,
                        help="Enable verbose output")
    # TODO:
    #   (-T, --timeout) socket timeout
    #   (-i, --input) manifest input file
    
    options, args = parser.parse_args()

    # Check input
    aws_key = os.environ.get("AWS_ACCESS_KEY_ID")
    aws_secret_key = os.environ.get("AWS_SECRET_ACCESS_KEY")

    ## AWS
    if options.aws_key:
        aws_key = options.aws_key
    if options.aws_secret_key:
        aws_secret_key = options.aws_secret_key
    if None in [aws_key, aws_secret_key]:
        parser.error("Missing required arguments `aws_key` or `aws_secret_key`")

    ## Threads
    if options.numthreads < 1:
        parser.error("`theads` must be at least 1")

    ## Arguments
    if len(args) < 1:
        parser.error("BUCKET not specified")
    bucket = args[0]
    if len(args) < 2:
        parser.error("OPERATION not specified")
    operation = args[1].lower()
    if operation not in ["get", "put", "list", "delete"]:
        parser.error("OPERATION must be one of: get, put, list, or delete")

    # Setup factories
    def toolbox_factory():
        return S3ToolBox(aws_key, aws_secret_key, bucket)
    def worker_factory(job_queue):
        return workerpool.EquippedWorker(job_queue, toolbox_factory)

    # Setup logging
    # TODO: Rewrite this to use different handlers
    level = logging.INFO
    if options.verbose:
        level = logging.DEBUG
    log.addHandler(logging.StreamHandler())
    log.level = level

    # Setup operation mapping
    job_map = {'get': GetJob}
    method_map = {'list': perform_list}

    if operation in method_map:
        return method_map[operation](toolbox_factory())
    
    if operation not in job_map:
        parser.error("Operation `%s' not implemented yet." % operation)

    # Get data source
    input_src = None
    if options.input:
        # Get source from manifest or stdin (via -i flag)
        if options.input == '-':
            input_src = "stdin"
            options.input = sys.stdin
        try:
            data = open(options.input)
        except IOError:
            log.error("%s: No such file" % options.input)
            return
        input_src = "`%s'" % options.input
    elif len(args) < 3:
        # Get source from stdin
        input_src = "stdin"
        data = sys.stdin
    else:
        if operation == 'put':
            # Get source from glob-expanded arguments
            data = []
            for arg in args[2:]:
                found = glob(arg)
                if not found:
                    log.error("%s: No such file." % arg)
                    continue
                data += found
        else:
            data = args[2:]
        input_src = "arguments: %s" % ', '.join(data)

    # Setup operation configuration
    config = {'acl': options.acl,
              'start_key': options.start_key,
              }

    # Fire up the thread pool
    log.info("Starting pool with %d threads..." % options.numthreads)
    try:
        pool = workerpool.WorkerPool(options.numthreads, maxjobs=options.numthreads*2, worker_factory=worker_factory)
    except Exception, e:
        log.critical("Failed starting workers: <%s> %s: %s" % (e.__class__.__name__, e.status, e.reason))
        return

    # Setup interrupt handling
    def shutdown(signum, stack):
        log.warn("Interrupted, shutting down...")
        event_stop.set()
    signal.signal(signal.SIGINT, shutdown)

    # Start feeding jobs into the workers
    log.info("Using files from %s" % input_src )
    Job = job_map[operation]
    try:
        for item in data:
            if event_stop.isSet(): break
            j = Job(item.strip())
            pool.put(j)
    except IOError:
        log.warn("Aborted.")

    pool.shutdown()

if __name__ == "__main__":
    main()
